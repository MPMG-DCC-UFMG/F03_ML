from item.clustering.utils import *
from item.clustering.item_representation import *
from nlp.utils import (
    read_json_file,
    plot_histogram,
    get_completetext,
    plot_wordcloud,
    print_statistics)
from nlp.preprocessing import (
    get_stopwords,
    tokenize_document,
    spellcheck_document,
    lemmatization_document
)
from .pricing import get_prices_statistics_df


def get_item_vec(_item, word_embeddings, word_class, categories=None,
                 embedding_type=None, norm=True, operation='mean'):
    '''
        Build the vector representation for an item using the word embeddings.

        _items (item): item object.
        word_embeddings (dict): pre-trained word embeddings (word -> embedding).
        word_class (dict): part of a speech tags (word -> tag).
        categories (list): word categories to be used.
        embedding_type (list): word tags to be used.
        norm (bool): if the item embeddings/vectors should be normalized.
        operation (str): operation to be used to build the item embeddings/vectors.
    '''

    if operation == 'mean':
        item_vec = get_item_embedding(_item.get_item_dict(), word_embeddings, word_class, \
                    categories=categories, embedding_type=embedding_type)
    elif operation == 'weighted':
        item_vec = get_item_embedding_weighted(_item.get_item_dict(), word_embeddings, word_class, \
                    categories=categories, embedding_type=embedding_type)
    elif operation == 'concatenate':
        item_vec = get_words_plus_categories_embeddings(_item.get_item_dict(), word_embeddings, word_class, \
                    categories=categories, embedding_type=embedding_type)

    if norm:
        item_vec = normalize(item_vec.reshape(1, -1))

    return item_vec


def pricing_item(description, word_embeddings, word_class, canonical_form,
                 reducer_model, clustering_model, items_clusters_df, categories=[],
                 embedding_type=['N', 'MED'], operation='mean', dsc_unidade=None,
                 year=None):
    '''
        Get item price statistics such as mean, median, max, min, first
        quantile, etc.


        description (str): description of the item.
        word_embeddings (dict): pre-trained word embeddings (word -> embedding).
        word_class (dict): part of a speech tags (word -> tag).
        canonical_form (dict): words' lemmas to use in lemmatization (word ->lemma).
        reducer_model (dict): dimensionality reduction models for each group generated
                             by the first token grouping.
        clustering_model (dict): HDBSCAN models for each group generated by the first
                                 token grouping.
        categories (list): word categories to be used.
        embedding_type (list): word tags to be used.
        operation (str): operation to be used to build the item embeddings/vectors.
        dsc_unidade (str): item's unit metric.
        year (str): item's year.
    '''

    # 1) TEXT CLEANING

    right_word = read_json_file('../dados/palavras/right_words_nilc.json')
    stopwords_ = get_stopwords()
    relevant_stopwords = {'para', 'com', 'nao', 'mais', 'muito', 'so', 'sem', \
                          'mesmo', 'mesma', 'ha', 'haja', 'hajam', 'houver', 'houvera', \
                          'seja', 'sejam', 'fosse', 'fossem', 'forem', 'sera', 'serao', \
                          'seria', 'seriam', 'tem', 'tinha', 'teve', 'tinham', 'tenha', \
                          'tiver', 'tiverem', 'tera', 'terao', 'teria', 'teriam', 'uma', \
                          'mais', 'entre'}

    stopwords_ = stopwords_ - relevant_stopwords

    # Preprocessing
    doc = preprocess_document(description, remove_numbers=False, stopwords=stopwords_)
    doc = tokenize_document(doc)
    doc = spellcheck_document(doc, right_word)
    doc = lemmatization_document(doc, canonical_form)

    # Categorization
    item = Item()
    itemslist = ItemList()
    item.extract_entities(doc, None, None, None, None, description, None, None,
                         itemslist.set_unit_metrics, itemslist.set_colors,
                         itemslist.set_materials, itemslist.set_sizes,
                         itemslist.set_quantities, itemslist.set_qualifiers,
                         itemslist.set_numbers)

    # 2) TEXT REPRESENTATION

    embedding_size = len(list(word_embeddings.values())[0])
    item_emb = get_item_vec(item, word_embeddings, word_class, categories=categories,
                            embedding_type=embedding_type, operation=operation)

    # 3) CLUSTERING

    item_dict = item.get_item_dict()
    group = item_dict['palavras'][0]

    # It gets the reduced vector for the item
    item_emb_red = reducer_model[group].transform(item_emb)
    # It gets the item cluster
    cluster = approximate_predict(clustering_model[group], item_emb_red)
    cluster = group + '_' + str(cluster[0][0])

    if dsc_unidade != None and year != None:
        items_clusters_df = items_clusters_df[(items_clusters_df.cluster == cluster) & \
                                          (items_clusters_df.dsc_unidade_medida == dsc_unidade) &
                                          (items_clusters_df.ano == year)]
    elif dsc_unidade != None:
        items_clusters_df = items_clusters_df[(items_clusters_df.cluster == cluster) & \
                                          (items_clusters_df.dsc_unidade_medida == dsc_unidade)]
    elif year != None:
        items_clusters_df = items_clusters_df[(items_clusters_df.cluster == cluster) & \
                                          (items_clusters_df.ano == year)]
    else:
        items_clusters_df = items_clusters_df[(items_clusters_df.cluster == cluster)]

    if len(items_clusters_df) == 0:
        statistics = {
            'cluster': cluster,
            'mean': -1,
            'median': -1,
            'var': -1,
            'std': -1
        }
        return statistics

    dsc_unidade = True if dsc_unidade != None else False
    year = True if year != None else False
    cluster_statistics = get_prices_statistics_df(items_clusters_df, dsc_unidade, year)
    statistics = cluster_statistics.iloc[0].to_dict()

    return statistics
