{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import collections\n",
    "import copy\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "from nlp.preprocessing import (\n",
    "    clean_text,\n",
    "    preprocess,\n",
    "    tokenize,\n",
    "    preprocess_document,\n",
    "    tokenize_document,\n",
    "    get_stopwords, \n",
    "    lemmatization_document,\n",
    "    get_canonical_words)\n",
    "from nlp.utils import (\n",
    "    plot_histogram,\n",
    "    get_completetext,\n",
    "    plot_wordcloud,\n",
    "    print_statistics,\n",
    "    groups_frequency_sort)\n",
    "from nlp.text_statistics import (\n",
    "    count_tokens,\n",
    "    unique_tokens\n",
    ")\n",
    "from nlp.grouping import (\n",
    "    get_groups,\n",
    "    get_groups_size,\n",
    "    get_unigram_groups,\n",
    "    get_two_tokens_groups,\n",
    "    get_first_token_groups,\n",
    "    get_bigram_groups,\n",
    "    get_first_two_groups,\n",
    "    groups_frequency_sort\n",
    ")\n",
    "from utils.read_files import (\n",
    "    get_items)\n",
    "from item.item_list import (\n",
    "    ItemList,\n",
    "    Item\n",
    ")\n",
    "from item.spellcheckeropt import SpellcheckerOpt\n",
    "from item.utils import get_tokens_set\n",
    "from textpp_ptbr.preprocessing import TextPreProcessing as tpp\n",
    "from gensim.parsing.preprocessing import (\n",
    "    strip_multiple_whitespaces,\n",
    "    strip_non_alphanum,\n",
    "    strip_punctuation2,\n",
    "    strip_short)\n",
    "\n",
    "#Import xmeans through pyclustering library:\n",
    "from pyclustering.cluster.center_initializer import kmeans_plusplus_initializer;\n",
    "from pyclustering.cluster.xmeans import xmeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# It gets the descpitons processed:\n",
    "itemlist = ItemList()\n",
    "itemlist.load_items_from_file('../dados/items_preprocessed.zip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#It gets the list of preprocessed descriptions:\n",
    "items_list = itemlist.items_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# It gets the first tokens of each description and groups\n",
    "# based on this approach:\n",
    "first_token_groups = itemlist.get_first_token_groups()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18035"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# total number of groups:\n",
    "len(first_token_groups)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#It creates a list of the the keys of these groups:\n",
    "groups = list(first_token_groups.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# It gets the values of each group (i.e., the id of the descriptions into that group):\n",
    "group_descriptions = list(first_token_groups.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0,\n",
       " 5397,\n",
       " 6121,\n",
       " 9458,\n",
       " 15702,\n",
       " 19193,\n",
       " 19735,\n",
       " 20034,\n",
       " 21082,\n",
       " 22880,\n",
       " 25711,\n",
       " 28490,\n",
       " 32120,\n",
       " 32182,\n",
       " 32944,\n",
       " 36888,\n",
       " 38819,\n",
       " 41181,\n",
       " 41762,\n",
       " 43768,\n",
       " 45599,\n",
       " 49124,\n",
       " 51157,\n",
       " 56107,\n",
       " 56897,\n",
       " 59936,\n",
       " 59959,\n",
       " 60560,\n",
       " 67022,\n",
       " 67167,\n",
       " 70748,\n",
       " 74153,\n",
       " 76287,\n",
       " 76855,\n",
       " 78235,\n",
       " 82736,\n",
       " 83447,\n",
       " 85959,\n",
       " 88728,\n",
       " 90844,\n",
       " 91238,\n",
       " 94270,\n",
       " 95632,\n",
       " 96928,\n",
       " 98906,\n",
       " 100731,\n",
       " 102871,\n",
       " 107503,\n",
       " 108234,\n",
       " 114893,\n",
       " 118373,\n",
       " 119705,\n",
       " 122732,\n",
       " 123123,\n",
       " 123816,\n",
       " 127104,\n",
       " 131248,\n",
       " 142926,\n",
       " 146718,\n",
       " 153824,\n",
       " 157309,\n",
       " 162036,\n",
       " 162417,\n",
       " 162958,\n",
       " 163312,\n",
       " 168818,\n",
       " 175315,\n",
       " 176437,\n",
       " 177094,\n",
       " 181874,\n",
       " 182344,\n",
       " 183951,\n",
       " 184123,\n",
       " 184784,\n",
       " 188475,\n",
       " 188843,\n",
       " 189925,\n",
       " 195327,\n",
       " 198283,\n",
       " 199351,\n",
       " 199677,\n",
       " 200842,\n",
       " 205269,\n",
       " 208983,\n",
       " 209647,\n",
       " 214597,\n",
       " 221900,\n",
       " 222282,\n",
       " 223146,\n",
       " 225666,\n",
       " 225894,\n",
       " 226972,\n",
       " 227348,\n",
       " 227692,\n",
       " 232618,\n",
       " 232920,\n",
       " 239366,\n",
       " 240420,\n",
       " 244013,\n",
       " 250111,\n",
       " 251733,\n",
       " 252427,\n",
       " 253254,\n",
       " 254081,\n",
       " 254262,\n",
       " 254580,\n",
       " 254934,\n",
       " 256490,\n",
       " 257845,\n",
       " 257975,\n",
       " 258586,\n",
       " 258846,\n",
       " 260102,\n",
       " 260592,\n",
       " 264120,\n",
       " 269070,\n",
       " 274308,\n",
       " 274905,\n",
       " 275492,\n",
       " 276075,\n",
       " 276964,\n",
       " 281526,\n",
       " 287618,\n",
       " 299727,\n",
       " 308599,\n",
       " 310263,\n",
       " 311138,\n",
       " 312299,\n",
       " 314436,\n",
       " 319636,\n",
       " 320896,\n",
       " 323880,\n",
       " 324267,\n",
       " 332188,\n",
       " 336644,\n",
       " 341360,\n",
       " 341969,\n",
       " 342613,\n",
       " 343718,\n",
       " 344253,\n",
       " 347475,\n",
       " 347535,\n",
       " 348228,\n",
       " 353751,\n",
       " 355258,\n",
       " 356287,\n",
       " 358345,\n",
       " 365562,\n",
       " 365729,\n",
       " 367627,\n",
       " 368932,\n",
       " 373605,\n",
       " 382268,\n",
       " 384739,\n",
       " 389154,\n",
       " 396787,\n",
       " 398833,\n",
       " 405746,\n",
       " 406227,\n",
       " 406415,\n",
       " 406582,\n",
       " 408066,\n",
       " 413631,\n",
       " 413968,\n",
       " 414740,\n",
       " 417506,\n",
       " 420854,\n",
       " 421628,\n",
       " 423254,\n",
       " 426028,\n",
       " 426725,\n",
       " 428049,\n",
       " 429952,\n",
       " 430711,\n",
       " 431613,\n",
       " 432117,\n",
       " 432938,\n",
       " 435036,\n",
       " 436150,\n",
       " 436845,\n",
       " 443609,\n",
       " 451743,\n",
       " 452136,\n",
       " 457508,\n",
       " 458194,\n",
       " 459708,\n",
       " 461181,\n",
       " 461254,\n",
       " 462284,\n",
       " 463009,\n",
       " 469027,\n",
       " 469799,\n",
       " 470622,\n",
       " 470927,\n",
       " 471245,\n",
       " 471421,\n",
       " 471775,\n",
       " 473603,\n",
       " 478351,\n",
       " 478930,\n",
       " 481544,\n",
       " 483569,\n",
       " 484562,\n",
       " 484878,\n",
       " 488479,\n",
       " 494447,\n",
       " 496026,\n",
       " 496971,\n",
       " 497096,\n",
       " 497133,\n",
       " 498744,\n",
       " 501105,\n",
       " 501276,\n",
       " 515955,\n",
       " 524950,\n",
       " 527049,\n",
       " 531855,\n",
       " 535925,\n",
       " 541272,\n",
       " 542869,\n",
       " 543911,\n",
       " 550906,\n",
       " 551631,\n",
       " 553777,\n",
       " 556035,\n",
       " 558796,\n",
       " 562616,\n",
       " 563362,\n",
       " 568004,\n",
       " 570468,\n",
       " 575484,\n",
       " 579002,\n",
       " 580972,\n",
       " 583499,\n",
       " 583899,\n",
       " 584015,\n",
       " 584596,\n",
       " 585562,\n",
       " 585613,\n",
       " 586663,\n",
       " 587423,\n",
       " 588120,\n",
       " 590613,\n",
       " 591552,\n",
       " 595319,\n",
       " 598365,\n",
       " 599660,\n",
       " 608821,\n",
       " 610016,\n",
       " 613377,\n",
       " 620335,\n",
       " 623735,\n",
       " 624213,\n",
       " 625083,\n",
       " 627378,\n",
       " 627916,\n",
       " 634941,\n",
       " 636433,\n",
       " 637239,\n",
       " 640562,\n",
       " 643632,\n",
       " 646597,\n",
       " 647855,\n",
       " 647966,\n",
       " 649083,\n",
       " 652675,\n",
       " 652957,\n",
       " 661985,\n",
       " 663140,\n",
       " 663304,\n",
       " 672963,\n",
       " 681497,\n",
       " 684151,\n",
       " 684285,\n",
       " 685394,\n",
       " 693039,\n",
       " 703741,\n",
       " 708331,\n",
       " 708759,\n",
       " 710601,\n",
       " 712176,\n",
       " 713738,\n",
       " 713832,\n",
       " 721162,\n",
       " 729372,\n",
       " 735763,\n",
       " 739779,\n",
       " 741888,\n",
       " 748029,\n",
       " 750252,\n",
       " 751077,\n",
       " 752471,\n",
       " 757501,\n",
       " 757552,\n",
       " 760628,\n",
       " 762185,\n",
       " 764337,\n",
       " 764621,\n",
       " 765370,\n",
       " 769147,\n",
       " 771636,\n",
       " 773473,\n",
       " 784308,\n",
       " 787279,\n",
       " 787871,\n",
       " 789105,\n",
       " 790640,\n",
       " 794832,\n",
       " 801412,\n",
       " 804361,\n",
       " 805053,\n",
       " 806513,\n",
       " 809286,\n",
       " 811049,\n",
       " 828166,\n",
       " 830991,\n",
       " 832552,\n",
       " 832807,\n",
       " 839441,\n",
       " 840013,\n",
       " 840830,\n",
       " 843272,\n",
       " 846511,\n",
       " 850025,\n",
       " 852224,\n",
       " 859645,\n",
       " 861383,\n",
       " 866119,\n",
       " 867967,\n",
       " 871495,\n",
       " 873569,\n",
       " 873769,\n",
       " 876659,\n",
       " 877717,\n",
       " 887423,\n",
       " 888084,\n",
       " 891334,\n",
       " 893789,\n",
       " 897822,\n",
       " 902055,\n",
       " 903458,\n",
       " 905613,\n",
       " 908198,\n",
       " 908297,\n",
       " 909799,\n",
       " 911510,\n",
       " 913627,\n",
       " 915699,\n",
       " 916358,\n",
       " 917303,\n",
       " 917855,\n",
       " 920562,\n",
       " 921718,\n",
       " 927372,\n",
       " 929423,\n",
       " 935079,\n",
       " 935442,\n",
       " 936614,\n",
       " 940073,\n",
       " 945470,\n",
       " 949219,\n",
       " 952898,\n",
       " 953817,\n",
       " 954252,\n",
       " 954525,\n",
       " 956391,\n",
       " 958210,\n",
       " 962275,\n",
       " 963392,\n",
       " 963990,\n",
       " 964222,\n",
       " 964241,\n",
       " 971760,\n",
       " 972953,\n",
       " 975894,\n",
       " 976137,\n",
       " 978660,\n",
       " 979620,\n",
       " 981745,\n",
       " 982714,\n",
       " 982907,\n",
       " 985862,\n",
       " 986577,\n",
       " 988522,\n",
       " 991738,\n",
       " 995754,\n",
       " 1009783,\n",
       " 1015518,\n",
       " 1017320,\n",
       " 1018785,\n",
       " 1021567,\n",
       " 1023957,\n",
       " 1026978,\n",
       " 1028512,\n",
       " 1030804,\n",
       " 1032679,\n",
       " 1032861,\n",
       " 1037815,\n",
       " 1038756,\n",
       " 1038951,\n",
       " 1040731,\n",
       " 1042545,\n",
       " 1042708,\n",
       " 1046245,\n",
       " 1046550,\n",
       " 1046560,\n",
       " 1049595,\n",
       " 1052561,\n",
       " 1055731,\n",
       " 1058851,\n",
       " 1059573,\n",
       " 1059680,\n",
       " 1060409,\n",
       " 1060549,\n",
       " 1060624,\n",
       " 1061969,\n",
       " 1062576,\n",
       " 1064420,\n",
       " 1065353,\n",
       " 1067398,\n",
       " 1068612,\n",
       " 1069360,\n",
       " 1071631,\n",
       " 1072081,\n",
       " 1072945,\n",
       " 1073346,\n",
       " 1074201,\n",
       " 1077451,\n",
       " 1081005,\n",
       " 1081096,\n",
       " 1084452,\n",
       " 1086575,\n",
       " 1087586,\n",
       " 1088151,\n",
       " 1089579,\n",
       " 1091422,\n",
       " 1100539,\n",
       " 1101259,\n",
       " 1102692,\n",
       " 1103580,\n",
       " 1107860,\n",
       " 1112228,\n",
       " 1114928,\n",
       " 1118565,\n",
       " 1121948,\n",
       " 1127068,\n",
       " 1127721,\n",
       " 1128719,\n",
       " 1137134,\n",
       " 1138340,\n",
       " 1138929,\n",
       " 1143974,\n",
       " 1144610,\n",
       " 1147008,\n",
       " 1147936,\n",
       " 1149329,\n",
       " 1156184,\n",
       " 1159567,\n",
       " 1161779,\n",
       " 1166769,\n",
       " 1170930,\n",
       " 1173183,\n",
       " 1176079,\n",
       " 1177121,\n",
       " 1180251,\n",
       " 1182128,\n",
       " 1187105,\n",
       " 1189793,\n",
       " 1192541,\n",
       " 1199006,\n",
       " 1199918,\n",
       " 1207282,\n",
       " 1210007,\n",
       " 1210580,\n",
       " 1214286,\n",
       " 1221587,\n",
       " 1224967,\n",
       " 1226578,\n",
       " 1226668,\n",
       " 1230730,\n",
       " 1235335,\n",
       " 1238031,\n",
       " 1243837,\n",
       " 1245617,\n",
       " 1246689,\n",
       " 1247018,\n",
       " 1248836,\n",
       " 1249109,\n",
       " 1251295,\n",
       " 1254558,\n",
       " 1257806,\n",
       " 1258801,\n",
       " 1261544,\n",
       " 1267151,\n",
       " 1267984,\n",
       " 1280556,\n",
       " 1283397,\n",
       " 1286976,\n",
       " 1294141,\n",
       " 1295528,\n",
       " 1296295,\n",
       " 1298190,\n",
       " 1300282,\n",
       " 1303745,\n",
       " 1305093,\n",
       " 1305273,\n",
       " 1306994,\n",
       " 1308289,\n",
       " 1311854,\n",
       " 1315832,\n",
       " 1317262,\n",
       " 1317898,\n",
       " 1322929,\n",
       " 1327404,\n",
       " 1330668,\n",
       " 1330701,\n",
       " 1331016,\n",
       " 1333013,\n",
       " 1333456,\n",
       " 1337304,\n",
       " 1343097,\n",
       " 1343433,\n",
       " 1348298,\n",
       " 1349163,\n",
       " 1352225,\n",
       " 1354709,\n",
       " 1355913,\n",
       " 1356844,\n",
       " 1362900,\n",
       " 1364720,\n",
       " 1365667,\n",
       " 1367844,\n",
       " 1368401,\n",
       " 1370064,\n",
       " 1376542,\n",
       " 1380046,\n",
       " 1383582,\n",
       " 1386651,\n",
       " 1387936,\n",
       " 1388161,\n",
       " 1388564,\n",
       " 1395263,\n",
       " 1397981,\n",
       " 1398677,\n",
       " 1416200,\n",
       " 1416372,\n",
       " 1422919,\n",
       " 1423428,\n",
       " 1423720,\n",
       " 1427117,\n",
       " 1431419,\n",
       " 1434030,\n",
       " 1436093,\n",
       " 1436860,\n",
       " 1439112,\n",
       " 1442207,\n",
       " 1446018,\n",
       " 1451229,\n",
       " 1451898,\n",
       " 1453253,\n",
       " 1458268,\n",
       " 1459039,\n",
       " 1467282,\n",
       " 1471898,\n",
       " 1480774,\n",
       " 1485809,\n",
       " 1486146,\n",
       " 1487201,\n",
       " 1492499,\n",
       " 1495677,\n",
       " 1497639,\n",
       " 1499746,\n",
       " 1499774,\n",
       " 1503460,\n",
       " 1505122,\n",
       " 1505548,\n",
       " 1505886]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "groups[0]\n",
    "group_descriptions[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the list of words (medicines and nouns) from the list of descriptions\n",
    "# in a specific group:\n",
    "def get_list_of_words(group_desc):\n",
    "    list_words = list()\n",
    "    medical= get_tokens_set('../dados/palavras/medications.txt')\n",
    "    canonical_form, word_class = get_canonical_words()\n",
    "    \n",
    "    for desc_id in group_desc:\n",
    "        words = itemlist.items_list[desc_id].get_item_dict()['palavras']\n",
    "        for p in words:\n",
    "            if((p not in list_words)): \n",
    "                if ((p in medical) or ((p in word_class) and (word_class[p] == 'N'))):\n",
    "                    list_words.append(p)\n",
    "                \n",
    "    list_words.sort()\n",
    "    \n",
    "    return list_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a zero matrix based on the size of the number \n",
    "# of descriptions in that group (row) and the number of \n",
    "# words (only medicines and nouns) from all descriptions\n",
    "# in that group:\n",
    "def define_zero_matrix(group_desc):\n",
    "    list_words = get_list_of_words(group_desc)\n",
    "    rows = len(group_desc)\n",
    "    columns = len(list_words)\n",
    "    print(str(rows) + ',' + print(columns))\n",
    "    matrix_bow = np.zeros((rows, columns))\n",
    "    \n",
    "    return matrix_bow, list_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the number of rows and columns generated by the Bag-of-Words\n",
    "def get_size_rows_columns(group_desc):\n",
    "    list_words = get_list_of_words(group_desc)\n",
    "    rows = len(group_desc)\n",
    "    columns = len(list_words)\n",
    "    \n",
    "    return rows, columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the bag-of-words matrix:\n",
    "def define_description_bow(group_desc):\n",
    "    matrix_list = define_zero_matrix(group_desc)\n",
    "    zeros = matrix_list[0]\n",
    "    list_words = matrix_list[1]    \n",
    "    i = 0\n",
    "    for desc_id in group_desc:\n",
    "        words = itemlist.items_list[desc_id].get_item_dict()['palavras']           \n",
    "        for w in words:\n",
    "            if(w in list_words):\n",
    "                k = list_words.index(w)\n",
    "                zeros[i, k]  = 1.0\n",
    "        i = i + 1\n",
    "    return zeros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_on_first_token_groups_bow(first_token_groups, itemlist, it_thread, lower, upper, Return_Rows, Return_Cols):\n",
    "   \n",
    "    # It creates a list of the the keys of these groups:\n",
    "    groups = list(first_token_groups.keys())\n",
    "    # It gets the values of each group (i.e., the id of the descriptions into that group):\n",
    "    group_descriptions = list(first_token_groups.values())\n",
    "    # It defines the dictionary that will have the clustering with first token\n",
    "    # together with x-means considering a bag-of-words of the descriptions \n",
    "    # grouped by the first token approach:\n",
    "    first_token_plus_bow_xmeans = {}\n",
    "    # Iterator of the first token groups:\n",
    "    ft_it = lower\n",
    "    arr_cols = []\n",
    "    arr_rows = []\n",
    "\n",
    "    while ft_it <= upper:\n",
    "        if(len(group_descriptions[ft_it]) >= 30):\n",
    "            print(str(it_thread) + ': ' + str(ft_it) + '/' + str(upper))\n",
    "            # Bag of words for the group 0:\n",
    "            bow = get_size_rows_columns(group_descriptions[ft_it])\n",
    "            arr_rows.append(bow[0])\n",
    "            arr_cols.append(bow[1])\n",
    "            \n",
    "        ft_it = ft_it + 1\n",
    "        \n",
    "    Return_Rows[it_thread] = arr_rows\n",
    "    Return_Cols[it_thread] = arr_cols\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ranges(group_len, n_threads):\n",
    "    total_len = group_len\n",
    "    num_threads = n_threads\n",
    "    lower = []\n",
    "    upper = []\n",
    "    step = int(total_len/num_threads)\n",
    "\n",
    "    for k in range(num_threads):\n",
    "        lower.append(0)\n",
    "        upper.append(0)\n",
    "\n",
    "    lower[0] = 0\n",
    "    upper[0] = step\n",
    "  \n",
    "    i = 1\n",
    "    j = 0\n",
    "    while (i < num_threads):    \n",
    "        upper[i]  = upper[j] + step\n",
    "        lower[i]  = upper[j] +  1\n",
    "        if(i%2 != 0):\n",
    "            upper[i] = upper[i] + 1\n",
    "        \n",
    "        i = i + 1\n",
    "        j = j + 1\n",
    "        \n",
    "    upper[n_threads - 1] = upper[n_threads - 1] - 1\n",
    "    return lower, upper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read ranges\n",
      "([0, 2577, 5154, 7730, 10307, 12883, 15460], [2576, 5153, 7729, 10306, 12882, 15459, 18034])\n"
     ]
    }
   ],
   "source": [
    "import multiprocessing\n",
    "manager = multiprocessing.Manager()\n",
    "Return_Rows = manager.dict()\n",
    "Return_Cols = manager.dict()\n",
    "jobs = []\n",
    "n_threads = 7\n",
    "# It gets the first tokens of each description and groups\n",
    "# based on this approach:\n",
    "first_token_groups = itemlist.get_first_token_groups()\n",
    "group_len = len(first_token_groups)\n",
    "first_token_groups_new = {}\n",
    "keys_ft = list(first_token_groups.keys())\n",
    "\n",
    "random.shuffle(keys_ft)\n",
    "for k in keys_ft:\n",
    "    first_token_groups_new[k] = first_token_groups[k]\n",
    "    \n",
    "# It defines the ranges (of the groups) the threads will work on:\n",
    "thread_ranges = get_ranges(group_len, n_threads)\n",
    "print('Read ranges')\n",
    "print(thread_ranges) \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read ranges\n",
      "([0, 2, 4, 5, 7, 8, 10, 11, 13, 14, 16, 17, 19, 20], [1, 3, 4, 6, 7, 9, 10, 12, 13, 15, 16, 18, 19, 20])\n"
     ]
    }
   ],
   "source": [
    "import multiprocessing\n",
    "manager = multiprocessing.Manager()\n",
    "Return_Rows = manager.dict()\n",
    "Return_Cols = manager.dict()\n",
    "jobs = []\n",
    "n_threads = 14\n",
    "# It gets the first tokens of each description and groups\n",
    "# based on this approach:\n",
    "first_token_groups = itemlist.get_first_token_groups()\n",
    "group_len = 14\n",
    "first_token_groups_new = {}\n",
    "keys_ft = list(first_token_groups.keys())\n",
    "\n",
    "random.shuffle(keys_ft)\n",
    "for k in keys_ft:\n",
    "    first_token_groups_new[k] = first_token_groups[k]\n",
    "    \n",
    "# It defines the ranges (of the groups) the threads will work on:\n",
    "thread_ranges = get_ranges(group_len, n_threads)\n",
    "print('Read ranges')\n",
    "print(thread_ranges) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3: 6/6\n",
      "13: 20/20\n"
     ]
    }
   ],
   "source": [
    "for i in range(n_threads):\n",
    "    p = multiprocessing.Process(target=analyze_on_first_token_groups_bow, args=(first_token_groups_new, itemlist, i, thread_ranges[0][i], thread_ranges[1][i], Return_Rows, Return_Cols))\n",
    "    jobs.append(p)\n",
    "    p.start()\n",
    "\n",
    "for proc in jobs:\n",
    "    proc.join()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_res = []\n",
    "rows_res = []\n",
    "\n",
    "for i in range(n_threads):\n",
    "    cols_res = cols_res + Return_Cols[i]\n",
    "    rows_res = rows_res + Return_Rows[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "min cols: 15\n",
      "max cols: 84\n",
      "min rows: 71\n",
      "max rows: 71\n",
      "avg cols: 49.5\n",
      "stdev cols: 48.79036790187178\n",
      "avg rows: 71\n",
      "stdev rows: 0.0\n"
     ]
    }
   ],
   "source": [
    "import statistics \n",
    "print('min cols: ' + str(min(cols_res)))\n",
    "print('max cols: ' + str(max(cols_res)))\n",
    "print('min rows: ' + str(min(rows_res)))\n",
    "print('max rows: ' + str(max(rows_res)))\n",
    "\n",
    "print('avg cols: ' + str(statistics.mean(cols_res)))\n",
    "print('stdev cols: ' + str(statistics.stdev(cols_res)))\n",
    "print('avg rows: ' + str(statistics.mean(rows_res)))\n",
    "print('stdev rows: ' + str(statistics.stdev(rows_res)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[71, 71]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rows_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/18035\n",
      "2/18035\n",
      "3/18035\n",
      "4/18035\n",
      "5/18035\n",
      "6/18035\n",
      "7/18035\n",
      "8/18035\n",
      "9/18035\n",
      "10/18035\n",
      "11/18035\n",
      "12/18035\n",
      "13/18035\n",
      "14/18035\n",
      "15/18035\n",
      "16/18035\n",
      "17/18035\n",
      "18/18035\n",
      "19/18035\n",
      "20/18035\n",
      "21/18035\n",
      "22/18035\n",
      "23/18035\n",
      "24/18035\n",
      "25/18035\n",
      "26/18035\n",
      "27/18035\n",
      "28/18035\n",
      "29/18035\n",
      "30/18035\n",
      "31/18035\n",
      "32/18035\n",
      "33/18035\n",
      "34/18035\n",
      "35/18035\n",
      "36/18035\n",
      "37/18035\n",
      "38/18035\n",
      "39/18035\n",
      "40/18035\n",
      "41/18035\n",
      "42/18035\n",
      "43/18035\n",
      "44/18035\n",
      "45/18035\n",
      "46/18035\n",
      "47/18035\n",
      "48/18035\n",
      "49/18035\n",
      "50/18035\n",
      "51/18035\n",
      "52/18035\n",
      "53/18035\n",
      "54/18035\n",
      "55/18035\n",
      "56/18035\n",
      "57/18035\n",
      "58/18035\n",
      "59/18035\n",
      "60/18035\n",
      "61/18035\n",
      "62/18035\n",
      "63/18035\n",
      "64/18035\n",
      "65/18035\n",
      "66/18035\n",
      "67/18035\n",
      "68/18035\n",
      "69/18035\n",
      "70/18035\n",
      "71/18035\n",
      "72/18035\n",
      "73/18035\n",
      "74/18035\n",
      "75/18035\n",
      "76/18035\n",
      "77/18035\n",
      "78/18035\n",
      "79/18035\n",
      "80/18035\n",
      "81/18035\n"
     ]
    }
   ],
   "source": [
    "arr_col = []\n",
    "arr_row = []\n",
    "len_gd = len(group_descriptions)\n",
    "i = 1\n",
    "for gd in group_descriptions:\n",
    "    print(str(i) + '/' + str(len_gd))\n",
    "    \n",
    "    if(len(gd) > 30):\n",
    "        r, c = get_size_rows_columns(gd)\n",
    "        arr_row.append(r)\n",
    "        arr_col.append(c)\n",
    "    i = i + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('min col: ' + str(min(arr_col)))\n",
    "print('min row: ' + str(min(arr_row)))\n",
    "print('max col: ' + str(max(arr_col)))\n",
    "print('max row: ' + str(max(arr_row)))\n",
    "print('avg row: ' + str((sum(arr_row)/len(arr_row))))\n",
    "print('avg col: ' + str((sum(arr_col)/len(arr_col))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_value = 0 if len(somelist) == 0 else sum(somelist)/len(somelist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#It saves the bag of words into a file:\n",
    "np.savetxt(\"bow.csv\", bow, delimiter=\";\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# It gets the list of words from the group description[0]\n",
    "list_words = get_list_of_words(group_descriptions[0])\n",
    "list_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for desc_id in group_descriptions[0]:\n",
    "    words = itemlist.items_list[desc_id].get_item_dict()['palavras']\n",
    "    print(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#It runs xmeans on the bag of words and returns clusters:\n",
    "def cluster_by_xmeans(bow):\n",
    "    xmeans_instance = xmeans(bow, ccore=False)\n",
    "    xmeans_instance.process();\n",
    "    clusters = xmeans_instance.get_clusters();\n",
    "    \n",
    "    return clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate_id_to_descriptions(ids, descriptions_ids):\n",
    "    arr = []\n",
    "    \n",
    "    for i in ids:\n",
    "        arr.append(descriptions_ids[i])\n",
    "    return arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cluster_on_first_token_groups_bow(items_list):\n",
    "    # It emplys the first token approach to group the descriptions:\n",
    "    first_token_groups = itemlist.get_first_token_groups()\n",
    "    # It creates a list of the the keys of these groups:\n",
    "    groups = list(first_token_groups.keys())\n",
    "    # It gets the values of each group (i.e., the id of the descriptions into that group):\n",
    "    group_descriptions = list(first_token_groups.values())\n",
    "    # It defines the dictionary that will have the clustering with first token\n",
    "    # together with x-means considering a bag-of-words of the descriptions \n",
    "    # grouped by the first token approach:\n",
    "    first_token_plus_bow_xmeans = {}\n",
    "    # Iterator of the first token groups:\n",
    "    ft_it = 0\n",
    "    \n",
    "    while ft_it < len(groups):\n",
    "        if(len(group_descriptions[ft_it]) > 30):\n",
    "            # Bag of words for the group 0:\n",
    "            bow = define_description_bow(group_descriptions[ft_it])\n",
    "    \n",
    "            #It applies the clusters on the bow of the descriptions - group 0:\n",
    "            clusters_bow = cluster_by_xmeans(bow)\n",
    "            it = 0\n",
    "            for c in clusters_bow:\n",
    "                # It translates ids from x-means to actual descriptions (new groups):\n",
    "                desc_ids = translate_id_to_descriptions(c, group_descriptions[ft_it])\n",
    "                # It defines the key of the map:\n",
    "                new_key = groups[ft_it] + '_' + str(it)\n",
    "                # It sets the maps:\n",
    "                first_token_plus_bow_xmeans[new_key] = desc_ids\n",
    "                it = it + 1\n",
    "        else:\n",
    "            first_token_plus_bow_xmeans[groups[ft_it]] = group_descriptions[ft_it]\n",
    "        \n",
    "        ft_it = ft_it + 1\n",
    "        \n",
    "    return first_token_plus_bow_xmeans"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
