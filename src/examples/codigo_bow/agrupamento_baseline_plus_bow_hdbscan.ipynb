{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing common libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import collections\n",
    "import copy\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#Importing text preprocessing methods:\n",
    "from nlp.preprocessing import (\n",
    "    clean_text,\n",
    "    preprocess,\n",
    "    tokenize,\n",
    "    preprocess_document,\n",
    "    tokenize_document,\n",
    "    get_stopwords, \n",
    "    lemmatization_document,\n",
    "    get_canonical_words)\n",
    "from textpp_ptbr.preprocessing import TextPreProcessing as tpp\n",
    "from gensim.parsing.preprocessing import (\n",
    "    strip_multiple_whitespaces,\n",
    "    strip_non_alphanum,\n",
    "    strip_punctuation2,\n",
    "    strip_short)\n",
    "\n",
    "#Importing libraries to check spelling:\n",
    "from item.spellcheckeropt import SpellcheckerOpt\n",
    "from item.utils import get_tokens_set\n",
    "\n",
    "\n",
    "#Importing text analysis:\n",
    "from nlp.utils import (\n",
    "    plot_histogram,\n",
    "    get_completetext,\n",
    "    plot_wordcloud,\n",
    "    print_statistics,\n",
    "    groups_frequency_sort)\n",
    "\n",
    "#Importing text statistics:\n",
    "from nlp.text_statistics import (\n",
    "    count_tokens,\n",
    "    unique_tokens\n",
    ")\n",
    "\n",
    "#Importing baseline approaches for clustering:\n",
    "from nlp.grouping import (\n",
    "    get_groups,\n",
    "    get_groups_size,\n",
    "    get_unigram_groups,\n",
    "    get_two_tokens_groups,\n",
    "    get_first_token_groups,\n",
    "    get_bigram_groups,\n",
    "    get_first_two_groups,\n",
    "    groups_frequency_sort\n",
    ")\n",
    "\n",
    "#Importing the stucture of the descriptions:\n",
    "from utils.read_files import (\n",
    "    get_items)\n",
    "from item.item_list import (\n",
    "    ItemList,\n",
    "    Item\n",
    ")\n",
    "\n",
    "#Importing xmeans through pyclustering library:\n",
    "from pyclustering.cluster.center_initializer import kmeans_plusplus_initializer;\n",
    "from pyclustering.cluster.xmeans import xmeans\n",
    "\n",
    "#Importing the HDBSCAN stand-alone method:\n",
    "import hdbscan\n",
    "\n",
    "\n",
    "#Importing the multiprocessing library:\n",
    "import multiprocessing\n",
    "\n",
    "#Importing the libraries to save the final resutls and making it possible to load them:\n",
    "import nltk\n",
    "import pickle\n",
    "\n",
    "\n",
    "#Get the list of words (medicines and nouns) from the list of descriptions\n",
    "#in a specific group.\n",
    "def get_list_of_words(group_desc, itemlist, medicines, canonical_form, word_class):\n",
    "    list_words = list()\n",
    "   \n",
    "    for desc_id in group_desc:\n",
    "        words = itemlist.items_list[desc_id].get_item_dict()['palavras']\n",
    "        for p in words:\n",
    "            if((p not in list_words)): \n",
    "                if ((p in medicines) or ((p in word_class) and (word_class[p] == 'N'))):\n",
    "                    list_words.append(p)\n",
    "                \n",
    "    list_words.sort()\n",
    "    \n",
    "    return list_words\n",
    "\n",
    "#Define a zero matrix based on the size of the number \n",
    "#of descriptions in that group (row) and the number of \n",
    "#words (only medicines and nouns) from all descriptions\n",
    "#in that group.\n",
    "def define_zero_matrix(group_desc, itemlist, medicines, canonical_form, word_class):\n",
    "    list_words = get_list_of_words(group_desc, itemlist, medicines, canonical_form, word_class)\n",
    "    rows = len(group_desc)\n",
    "    columns = len(list_words)\n",
    "    matrix_bow = np.zeros((rows, columns))\n",
    "    \n",
    "    return matrix_bow, list_words, rows, columns\n",
    "\n",
    "# Define the bag-of-words matrix.\n",
    "def define_description_bow(group_desc, itemlist, medicines, canonical_form, word_class):\n",
    "    matrix_list = define_zero_matrix(group_desc, itemlist, medicines, canonical_form, word_class)\n",
    "    zeros = matrix_list[0]\n",
    "    list_words = matrix_list[1]\n",
    "    rows = matrix_list[2]\n",
    "    columns = matrix_list[3]   \n",
    "    i = 0\n",
    "    for desc_id in group_desc:\n",
    "        words = itemlist.items_list[desc_id].get_item_dict()['palavras']           \n",
    "        for w in words:\n",
    "            if(w in list_words):\n",
    "                k = list_words.index(w)\n",
    "                zeros[i, k]  = 1.0\n",
    "        i = i + 1\n",
    "    return zeros, rows, columns\n",
    "\n",
    "#It applies x-means on the bag of words.\n",
    "def cluster_by_xmeans(bow, number_of_descriptions):\n",
    "    cluster_size_limit = round(number_of_descriptions/30)\n",
    "    xmeans_instance = xmeans(bow, kmax=cluster_size_limit, ccore=False)\n",
    "    xmeans_instance.process();\n",
    "    clusters = xmeans_instance.get_clusters();\n",
    "    \n",
    "    return clusters\n",
    "\n",
    "#It just transfors the sklearn output to the pyclustering output\n",
    "#as they differ in terms of representation.\n",
    "def transform_sklearn_to_pyclustering(output):\n",
    "    output_dict = {}\n",
    "    i = 0\n",
    "    \n",
    "    while i < len(output):     \n",
    "        if(output[i] not in output_dict):\n",
    "            aux_arr = []\n",
    "            aux_arr.append(i)\n",
    "            output_dict[output[i]] = aux_arr\n",
    "        else:\n",
    "            aux_arr = output_dict[output[i]]\n",
    "            aux_arr.append(i)\n",
    "            output_dict[output[i]] = aux_arr       \n",
    "        \n",
    "        i = i + 1\n",
    "        \n",
    "    output_arr = []\n",
    "    \n",
    "    for key in output_dict:\n",
    "        output_arr.append(output_dict[key])\n",
    "    \n",
    "    return output_arr\n",
    "\n",
    "\n",
    "#It applies hdbscan on the bag of words.\n",
    "def cluster_by_hdbscan(bow, employed_metric, groups_ft):\n",
    "    clusterer = hdbscan.HDBSCAN(min_cluster_size=30, metric=employed_metric)\n",
    "    cluster_labels = clusterer.fit_predict(bow)\n",
    "    cluster_labels_post = cluster_labels\n",
    "    i = 0\n",
    "\n",
    "    while (i < len(cluster_labels)):\n",
    "        if(cluster_labels[i] == -1):\n",
    "            cluster_labels_post[i] = groups_ft + '_' + str(cluster_labels[i])\n",
    "        i = i + 1\n",
    "\n",
    "    print('before changing the default output')\n",
    "    print(cluster_labels)\n",
    "\n",
    "    print('after changing the default output')\n",
    "    print(cluster_labels_post)\n",
    "    clusters = transform_sklearn_to_pyclustering(cluster_labels_post)\n",
    "    \n",
    "    print('building the final output')\n",
    "    print(clusters)\n",
    "\n",
    "    return clusters\n",
    "\n",
    "\n",
    "#It calls the specific method depending on 'cluster_option' parameter.\n",
    "#groups_ft is used when we have outliers, so we can separate these outliers\n",
    "#considering the groups (from the First Token approach) they actually represent.\n",
    "def general_clustering(bow, groups_ft, number_of_descriptions, cluster_option):\n",
    "    clusters = None\n",
    "    \n",
    "    #cluster_option = 0, it employs x_means with the Euclidean distance:\n",
    "    if(cluster_option == 0):\n",
    "       clusters = cluster_by_xmeans(bow, number_of_descriptions)\n",
    "    #cluster_option = 1, it employs  hdbscan with the Euclidean distance (normalized by l2):\n",
    "    elif(cluster_option == 1):\n",
    "        clusters = cluster_by_hdbscan(bow, 'l2', groups_ft)\n",
    "    #cluster_option = 1, it employs hdbscan with the Hamming distance:\n",
    "    elif(cluster_option == 2):\n",
    "        clusters = cluster_by_hdbscan(bow, 'hamming', groups_ft)\n",
    "    #otherwise,  it employs x_means with the Euclidean distance:\n",
    "    else:\n",
    "        clusters = cluster_by_xmeans(bow, number_of_descriptions)\n",
    "\n",
    "    return clusters\n",
    "\n",
    "\n",
    "#Translate the generated ids of the clustering approach to actual description ids.\n",
    "def translate_id_to_descriptions(ids, descriptions_ids):\n",
    "    arr = []\n",
    "    \n",
    "    for i in ids:\n",
    "        arr.append(descriptions_ids[i])\n",
    "    return arr\n",
    "\n",
    "\n",
    "#It clusters again the groups generated by the first token approach. For now, this method only accepts X-Means and HDBScan with specific characteristics.\n",
    "def cluster_on_first_token_groups_bow(first_token_groups, itemlist, it_thread, lower, upper, medicines, canonical_form, word_class, Return_dict, cluster_option):\n",
    "    print(it_thread)\n",
    "    #It creates a list of the the keys of these groups:\n",
    "    groups = list(first_token_groups.keys())\n",
    "    #It gets the values of each group (i.e., the id of the descriptions into that group):\n",
    "    group_descriptions = list(first_token_groups.values())\n",
    "    #It defines the dictionary that will have the clustering with first token\n",
    "    #together with x-means considering a bag-of-words of the descriptions \n",
    "    #grouped by the first token approach:\n",
    "    first_token_plus_bow_xmeans = {}\n",
    "    #Iterator of the first token groups:\n",
    "    ft_it = lower\n",
    "\n",
    "    while ft_it <= upper:\n",
    "        print(str(it_thread) + ': ' + str(ft_it) + '/' + str(upper))\n",
    "        #It only considers to cluster again if the number of descritptions of that group has more than 30 descriptions\n",
    "        if(len(group_descriptions[ft_it]) >= 30):\n",
    "            \n",
    "            #Bag of words for the group 0:\n",
    "            bow = define_description_bow(group_descriptions[ft_it], itemlist, medicines, canonical_form, word_class)\n",
    "            \n",
    "            #It only applies the traditional clustering methods if the number of rows and columns of the bow are greater than zero:\n",
    "            if(bow[1] > 0 and bow[2] > 0):        \n",
    "                #It applies the clusters on the bow of the descriptions - group 0:\n",
    "                \n",
    "                clusters_bow = general_clustering(bow[0], groups[ft_it], len(group_descriptions[ft_it]), cluster_option)\n",
    "                it = 0\n",
    "                for c in clusters_bow:\n",
    "                    #It translates ids from x-means to actual descriptions (new groups):\n",
    "                    desc_ids = translate_id_to_descriptions(c, group_descriptions[ft_it])\n",
    "                    #It defines the key of the map:\n",
    "                    new_key = groups[ft_it] + '_' + str(it)\n",
    "                    #It sets the maps:\n",
    "                    first_token_plus_bow_xmeans[new_key] = desc_ids\n",
    "                    it = it + 1\n",
    "            else:\n",
    "                first_token_plus_bow_xmeans[groups[ft_it]] = group_descriptions[ft_it]\n",
    "        else:\n",
    "            first_token_plus_bow_xmeans[groups[ft_it]] = group_descriptions[ft_it]\n",
    "        ft_it = ft_it + 1\n",
    "        \n",
    "    Return_dict[it_thread] = first_token_plus_bow_xmeans\n",
    "\n",
    "\n",
    "#It gets the ranges of the clusters generated by the First Token approach\n",
    "#This is done in order to the processes work on.\n",
    "def get_ranges(group_len, n_threads):\n",
    "    if(n_threads == 1):\n",
    "        return 0, (group_len - 1)\n",
    "\n",
    "    total_len = group_len\n",
    "    num_threads = n_threads\n",
    "    lower = []\n",
    "    upper = []\n",
    "    step = int(total_len/num_threads)\n",
    "\n",
    "    for k in range(num_threads):\n",
    "        lower.append(0)\n",
    "        upper.append(0)\n",
    "\n",
    "    lower[0] = 0\n",
    "    upper[0] = step\n",
    "  \n",
    "    i = 1\n",
    "    j = 0\n",
    "    while (i < num_threads):    \n",
    "        upper[i]  = upper[j] + step\n",
    "        lower[i]  = upper[j] +  1\n",
    "        if(i%2 != 0):\n",
    "            upper[i] = upper[i] + 1\n",
    "        \n",
    "        i = i + 1\n",
    "        j = j + 1\n",
    "    \n",
    "    #Please, check if the final cluster range ends with 18,034 clusters\n",
    "    #(i.e., the number of clusters generated by First Token).\n",
    "    #Depending of the number of processes, you may have to change this \"-1\"\n",
    "    #for something else.\n",
    "    upper[n_threads - 1] = upper[n_threads - 1] - 1 \n",
    "    return lower, upper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read data preprocessed\n"
     ]
    }
   ],
   "source": [
    "manager = multiprocessing.Manager()\n",
    "Return_dict = manager.dict()\n",
    "jobs = []\n",
    "n_threads = 10\n",
    "\n",
    "#It loads the medical terms (medicines, drugs, etc):\n",
    "medicines = get_tokens_set('../dados/palavras/medications.txt')\n",
    "#It loads the canonical forms and their classes\n",
    "canonical_form, word_class = get_canonical_words()\n",
    "\n",
    "#It gets the descpitons processed:\n",
    "itemlist = ItemList()\n",
    "itemlist.load_items_from_file('../dados/items_preprocessed.zip')\n",
    "#It gets the list of preprocessed descriptions:\n",
    "\n",
    "print('Read data preprocessed')\n",
    "#It gets the first tokens of each description and groups\n",
    "#based on this approach:\n",
    "first_token_groups = itemlist.get_first_token_groups()\n",
    "group_len = len(first_token_groups)\n",
    "first_token_groups_new = {}\n",
    "\n",
    "#It shuffles the itens based on their keys:\n",
    "keys_ft = list(first_token_groups.keys())\n",
    "random.shuffle(keys_ft)\n",
    "random.shuffle(keys_ft)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_option = 2\n",
    "\n",
    "#It fills another dictionary with the shuffled keys:\n",
    "for k in keys_ft:\n",
    "    first_token_groups_new[k] = first_token_groups[k]\n",
    "    \n",
    "#It defines the ranges (of the groups) the processes will work on:\n",
    "thread_ranges = get_ranges(group_len, n_threads)\n",
    "print('Read ranges')\n",
    "print(thread_ranges) \n",
    "    \n",
    "#It creates the processes (balanced by shuffling the keys of the dictionary:\n",
    "for i in range(n_threads):\n",
    "    p = multiprocessing.Process(target=cluster_on_first_token_groups_bow, args=(first_token_groups_new, itemlist, i, thread_ranges[0][i], thread_ranges[1][i], medicines, canonical_form, word_class, Return_dict, cluster_option))\n",
    "    jobs.append(p)\n",
    "    p.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#It joins the results\n",
    "for proc in jobs:\n",
    "    proc.join()\n",
    "    \n",
    "#It gets all the results of the processes by accessing the Return_dict of each process:\n",
    "dictionary_clusters = {}    \n",
    "for i in range(n_threads):\n",
    "    dictionary_clusters.update(Return_dict[i])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
