{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import random\n",
    "import math\n",
    "import json\n",
    "import multiprocessing\n",
    "import json\n",
    "from item.item_list import (\n",
    "    ItemList,\n",
    "    Item\n",
    ")\n",
    "from nlp.utils import (\n",
    "    plot_histogram,\n",
    "    get_completetext,\n",
    "    plot_wordcloud,\n",
    "    print_statistics,\n",
    "    groups_frequency_sort,\n",
    "    read_json_file,\n",
    "    get_tokens_set\n",
    ")\n",
    "from nlp.grouping import (\n",
    "    get_groups,\n",
    "    get_groups_size,\n",
    "    get_unigram_groups,\n",
    "    get_two_tokens_groups,\n",
    "    get_first_token_groups,\n",
    "    get_bigram_groups,\n",
    "    get_first_two_groups,\n",
    "    groups_frequency_sort\n",
    ")\n",
    "from nlp.pos_tagging import (\n",
    "    get_tokens_tags\n",
    ")\n",
    "from nlp.word_embeddings import (\n",
    "    load_word_embeddings,\n",
    "    get_item_embedding,\n",
    "    get_items_embeddings,\n",
    "    get_items_similarities\n",
    ")\n",
    "from nlp.spellcheckeropt import SpellcheckerOpt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading word embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  word embeddings file, each line contains an embedding\n",
    "word_embeddings_file = '../../../embeddings/cbow_s50.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read word embeddings from file and store them in a map\n",
    "word_embeddings = load_word_embeddings(word_embeddings_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# It gets the descpitons processed:\n",
    "itemlist = ItemList()\n",
    "itemlist.load_items_from_file('../dados/items_preprocessed.zip', just_words=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the tags of tokens descriptions\n",
    "word_class = get_tokens_tags()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = 0\n",
    "empty = 0\n",
    "\n",
    "for doc in itemlist.items_list:\n",
    "    flag = False\n",
    "    for tok in doc:\n",
    "        if tok not in word_class:\n",
    "            continue\n",
    "        elif word_class[tok] in {'N', 'MED'} and tok in word_embeddings:\n",
    "            flag = True\n",
    "    \n",
    "    if flag:\n",
    "        count += 1\n",
    "    else:\n",
    "        print(doc)\n",
    "        empty += 1\n",
    "\n",
    "print(count)\n",
    "print(empty)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(itemlist.items_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the vector representation for an item using the word embeddings\n",
    "items_embeddings = get_items_embeddings(itemlist.items_list, word_embeddings, word_class, embedding_type=['N', 'MED'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(items_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = 0\n",
    "items_woembedding = set()\n",
    "\n",
    "for i in range(0, len(items_embeddings)):\n",
    "    embedding = items_embeddings[i]\n",
    "    zero_vector = np.array(embedding) == np.zeros(len(embedding))\n",
    "    if zero_vector.all():\n",
    "        items_woembedding.add(i)\n",
    "        count += 1\n",
    "\n",
    "count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_token_groups = itemlist.get_first_token_groups(just_words=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(first_token_groups)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = './results/baseline+embeddings/embeddings50_SUB+MED_xmeans.pkl'\n",
    "a_file = open(file, \"rb\")\n",
    "output = pickle.load(a_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = 0\n",
    "groups_woembedding = set()\n",
    "\n",
    "for group, items in first_token_groups.items():\n",
    "    items_set = set(items)\n",
    "    intersection = items_set.intersection(items_woembedding)\n",
    "    if len(intersection) >= len(items_set) - 1:\n",
    "        groups_woembedding.add(group)\n",
    "        count += 1\n",
    "\n",
    "count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = 0\n",
    "\n",
    "for group, items in first_token_groups.items():\n",
    "    if group in groups_woembedding:\n",
    "        count += len(items)\n",
    "\n",
    "count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spellchecker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_words = itemlist.unique_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(unique_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_set_file = '../dados/palavras/words_nilc_preprocessed.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_set = set(read_json_file(words_set_file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "medical = get_tokens_set('../dados/palavras/medications.txt')\n",
    "medical = set(medical)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = 0\n",
    "tokens_woembedding = set()\n",
    "\n",
    "for token in unique_words:\n",
    "    if token not in word_embeddings and token not in words_set and token not in medical:\n",
    "        tokens_woembedding.add(token)\n",
    "        count += 1\n",
    "\n",
    "count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = 0\n",
    "\n",
    "for token in unique_words:\n",
    "    if token in word_embeddings and (token not in words_set and token not in medical):\n",
    "        count += 1\n",
    "\n",
    "count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_set = list(words_set) + list(medical) + list(word_embeddings.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spellchecker = SpellcheckerOpt()\n",
    "spellchecker.load_words(words_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_woembedding_similar = {}\n",
    "words_checked = 0\n",
    "\n",
    "distance = 2\n",
    "verbose = True\n",
    "\n",
    "for token in tokens_woembedding:\n",
    "    words_list = spellchecker.search(token, distance)\n",
    "    if len(words_list) > 0:\n",
    "        words_list.sort(key=lambda x:(x[1], x[0]))\n",
    "        token_woembedding_similar[token] = words_list[0][0]\n",
    "    words_checked += 1\n",
    "    if verbose and words_checked%1000 == 0:\n",
    "        print('%d words checked' % (words_checked))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(token_woembedding_similar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_woembedding_similar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = 0\n",
    "\n",
    "for token, similar in token_woembedding_similar.items():\n",
    "    if similar in word_embeddings:\n",
    "        count += 1\n",
    "\n",
    "count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../dados/palavras/right_words_nilc.json\", \"w\") as JFile:\n",
    "    json.dump(token_woembedding_similar, JFile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build word embedding from set of  public procurements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import FastText"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "licitacao_items = read_json_file(\"../dados/licitacao_items_preprocessed.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = list(licitacao_items.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "items_licitacao = []\n",
    "\n",
    "for licitcao, items in licitacao_items.items():\n",
    "    licitacao_items_list = []\n",
    "    for item in items:\n",
    "        licitacao_items_list += item\n",
    "    items_licitacao.append(licitacao_items_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(items_licitacao)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "items_licitacao[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = FastText(items_licitacao[:100], size=300, window=10, batch_words=1000, sg=1, workers=3, iter=20, min_count=0, word_ngrams=1)\n",
    "model.save(\"fasttext_s300.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
