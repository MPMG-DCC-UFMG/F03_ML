{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import collections\n",
    "import matplotlib.pyplot as plt\n",
    "import zipfile\n",
    "from nlp.preprocessing import (\n",
    "    clean_text,\n",
    "    preprocess,\n",
    "    tokenize,\n",
    "    preprocess_document,\n",
    "    tokenize_document,\n",
    "    get_stopwords,\n",
    "    lemmatization)\n",
    "from nlp.text_statistics import (\n",
    "    number_tokens,\n",
    "    tokens_length,\n",
    "    unique_tokens,\n",
    "    count_numbers,\n",
    "    number_stopwords,\n",
    "    print_statistics)\n",
    "from gensim.parsing.preprocessing import (\n",
    "    strip_multiple_whitespaces,\n",
    "    strip_non_alphanum,\n",
    "    strip_punctuation2,\n",
    "    strip_short)\n",
    "from textpp_ptbr.preprocessing import TextPreProcessing as tpp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = './dados/itens_pregao_pitem_saude.csv'\n",
    "data = pd.read_csv(file, sep=';')\n",
    "\n",
    "data.info()\n",
    "items = list(data['nom_item'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "items_descriptions = preprocess(items, True, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary_file = '../dados/dicionario/delaf.dic.zip'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "canonical_forms = collections.defaultdict(list)\n",
    "word_class = {}\n",
    "\n",
    "with zipfile.ZipFile(dictionary_file, 'r') as zipped:\n",
    "    with zipped.open('delaf.dic', 'r') as data:\n",
    "        lines = data.readlines()\n",
    "        for line in lines:\n",
    "            word_canonical = re.split(r'[,.+:\\s]\\s*', line)\n",
    "            word = tpp.remove_accents(word_canonical[0])\n",
    "            canonical = tpp.remove_accents(word_canonical[1])\n",
    "            wclass = word_canonical[2]\n",
    "            canonical_forms[word].append(canonical)\n",
    "            if canonical in word_class and wclass == 'N':\n",
    "                word_class[canonical] = wclass\n",
    "            elif canonical not in word_class:\n",
    "                word_class[canonical] = wclass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "canonical_form = {}\n",
    "\n",
    "for word, canonical in canonical_forms.items():\n",
    "    if len(canonical) == 1:\n",
    "        canonical_form[word] = canonical[0]\n",
    "    else:\n",
    "        noun = ''\n",
    "        for c in canonical:\n",
    "            if word == 'milho':\n",
    "                print(c)\n",
    "            if word_class[c] == 'N':\n",
    "                noun = c\n",
    "                break\n",
    "        if noun == '':\n",
    "            canonical_form[word] = canonical[0]\n",
    "        else:\n",
    "            canonical_form[word] = noun"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(canonical_form)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "descriptions_lemmatized = []\n",
    "changes = 0\n",
    "i = 0\n",
    "\n",
    "for doc in items_descriptions:\n",
    "    tokens = []\n",
    "    for tok in doc:\n",
    "        if tok in canonical_form:\n",
    "            tokens.append(canonical_form[tok])\n",
    "            if tok != canonical_form[tok]:\n",
    "                changes += 1\n",
    "        else:\n",
    "            tokens.append(tok)\n",
    "    if i < 1000:\n",
    "        i += 1\n",
    "        print(doc)\n",
    "        print(tokens)\n",
    "    descriptions_lemmatized.append(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(descriptions_lemmatized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
